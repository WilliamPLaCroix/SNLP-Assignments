{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLP Assignment 3\n",
    "\n",
    "Name 1: William LaCroix<br/>\n",
    "Student id 1: 7038732<br/>\n",
    "Email 1: williamplacroix@gmail.com<br/>\n",
    "\n",
    "\n",
    "Name 2: Nicholas Jennings<br/>\n",
    "Student id 2: 2573492<br/>\n",
    "Email 2: s8nijenn@stud.uni-saarland.de<br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the respective Python files for any additional exercises necessary. There is no need to submit the data files should they exist. <br/>\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2.zip**. Make sure to click on \"Turn-in\" (or the equivalent on CMS) after your upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Cross Entropy and KL-Divergence (5 points)\n",
    "\n",
    "A quick reminder on cross entropy, we define it as:\n",
    "\n",
    "$$ H(P,Q) =  -\\sum_{x \\in X} P(x) \\cdot \\log Q(x) $$\n",
    "\n",
    "Another metric (besides perplexity and cross-entropy) to compare two probability distributions is the Kullback-Leibler Divergence $D_{KL}$. It is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "D_{KL}(P\\|Q) = \\sum_{x \\in X}P(x) \\cdot \\log \\frac{P(x)}{Q(x)}\n",
    "\\end{equation}\n",
    "\n",
    "Where $P$ is the empirical or observed distribution, and Q is the estimated distribution over a common probabilitiy space $X$.\n",
    "\n",
    "As already explained, we use these two metrics to minimize the difference between an observed distribution and an estimation of the distribution. This will never be perfect, but we aim to minimize with the available information and resources.\n",
    "\n",
    "Answer the following questions regarding these two metrics:\n",
    "\n",
    "1. In the context of language modeling, briefly explain what $P$ and $Q$ are in the above expressions. Also, explain how can we compute cross-entropy in practice. (2 point)\n",
    "1. In the minimization problem mentioned above, i.e. minimizing the difference between the two distributions, is minimizing the cross-entropy, i.e. $ H(P,Q) $, and $D_{KL}(P\\|Q)$ equivalent? Support your answer with a mathematical expression. (2 point)\n",
    "1. In the lecture, it was mentioned that KL-Divergence is not a distance metric. Why is this? Provide a counter-example for the properties of a distance metric. (1 point)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 2 - Prefix coding (5 points)\n",
    "\n",
    "As explained in the lecture, a nice way of constructing a code would be, is to determine the length of the encoding a token based on the frequency of the token. This can be done in many ways. In the lecture we talked about prefix codes:\n",
    "\n",
    "* No code word is a prefix of another code word\n",
    "* We can organize the code as a tree\n",
    "\n",
    "1. Given an arbitrary alphabet along with probabilities for each token, you are to implement a function that outputs the encoding for each character. (3 points.)\n",
    "\n",
    "Hint: feel free to use the example in the slides to validate that your generated encoding is correct:\n",
    "\n",
    "| word | frequency | $C(\\text{word})$ |\n",
    "| ---- | --------- | ------- |\n",
    "|\"the\" | 0.5       | `0`     |\n",
    "|\"and\" | 0.25      | `10`    |\n",
    "|\"of\"  | 0.125     | `110`   |\n",
    "|\"he\"  | 0.125     | `111`   |\n",
    "\n",
    "Where $C(\\text{word})$ represents the encoding of `word`.\n",
    "\n",
    "Though this algorithm is generalizable to any base of the code (i.e. the code need not be binary), we shall limit this exercise to binary encoding.\n",
    "\n",
    "2. The type of encoding from above does not always achieve the optimal encoding given an alphabet and its associated set of distributions. Provide an example for which this algorithm fails to find the optimal coding (1 point)\n",
    "3. An algorithm called Huffman coding exists that can achieve an optimal encoding in every case. Describe briefly how it differs from the simple prefix coding we used for this exercise. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from bonus file\n",
    "from importlib import reload\n",
    "import exercise_2\n",
    "exercise_2 = reload(exercise_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case. Feel free to add more.\n",
    "encoding = exercise_2.get_encoding({'the': 0.5, 'and': 0.25, 'of': 0.125, 'he': 0.125})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus - Long range dependencies (2 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we saw how being able to handle long-range dependencies can be valuable. Not only that, but we also saw some tokens also rely on medium- or short-range dependencies.\n",
    "\n",
    "Modern models are quite good at capturing all of these dependencies. Or are they?! In this exercise we will be going over some simple examples and evaluating how modern language models can handle long-range dependencies.\n",
    "\n",
    "NOTE: For this task you will probably have to **rely on Google Colab in order to run the relevant code**.\n",
    "\n",
    "We will be using a light-weight (~120M parameters), but relatively modern language model: GPT-2. GPT-2 is a generative model which can is conditioned on its input in order to generate new tokens. In future lectures we will learn more about language modelling and how this model can be trained. For now we will only be using it very practically.\n",
    "\n",
    "This model will give us a distribution of probabilities over the tokens in the vocabulary of the model which indicate what it thinks the next word should be conditioned on the input you give it.\n",
    "\n",
    "Assuming a sequence $(w_1, w_2, \\dots, w_n)$, the model outputs a distribution for which this holds true:\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^N p(w_j|w_1, w_2, \\dots, w_{n-1}) = 1\n",
    "$$\n",
    "\n",
    "Where $N$ is the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries.\n",
    "# NOTE: Remember, run this on Google Colab (unless you know what you're doing)\n",
    "!pip install -q torch huggingface_hub tokenizers sentencepiece sacremoses importlib_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from bonus file\n",
    "from importlib import reload\n",
    "import bonus\n",
    "bonus = reload(bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model and tokenizer from huggingface.co or cache\n",
    "model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', 'gpt2')\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'gpt2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A familiar example\n",
    "\n",
    "We have provided you with the code to run inference on the model and be able to evaluate its output. However, **feel free to change the code if you are not satisfied with how it is set up initially**.\n",
    "\n",
    "In the lecture we discussed a repeating sequence where entropy goes to zero after a certain length is reached. How does that look in modern models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus.model_predict('A B C A B C', model, tokenizer, top_picks=5)\n",
    "\n",
    "# Expected output:\n",
    "# {'A': 0.42868340015411377, 'D': 0.14357835054397583, 'C': 0.13164187967777252, 'B': 0.08625581860542297, 'E': 0.01257376465946436}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict('A B C A B C A B C', model, tokenizer, 5)\n",
    "\n",
    "# Expected output:\n",
    "# {'A': 0.7761804461479187, 'C': 0.07017528265714645, 'B': 0.04758051782846451, 'D': 0.01468430832028389, 'E': 0.003714686958119273}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the conditional entropy would be going down as the probability distribution gets more and more skewed towards `A` if we condition the model on a longer repeating sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your task\n",
    "\n",
    "Come up with sequences where you can evaluate how the model deals with long-range dependencies. How would you estimate the conditional entropy of the model as you modify the phrases? Present and explain **at least** 2 examples (1+1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus.model_predict('TODO: YOUR PHRASE HERE', model, tokenizer, 5)\n",
    "# TODO: Add more examples here"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
