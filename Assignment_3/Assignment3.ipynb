{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLP Assignment 3\n",
    "\n",
    "Name 1: William LaCroix<br/>\n",
    "Student id 1: 7038732<br/>\n",
    "Email 1: williamplacroix@gmail.com<br/>\n",
    "\n",
    "\n",
    "Name 2: Nicholas Jennings<br/>\n",
    "Student id 2: 2573492<br/>\n",
    "Email 2: s8nijenn@stud.uni-saarland.de<br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the respective Python files for any additional exercises necessary. There is no need to submit the data files should they exist. <br/>\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2.zip**. Make sure to click on \"Turn-in\" (or the equivalent on CMS) after your upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Cross Entropy and KL-Divergence (5 points)\n",
    "\n",
    "A quick reminder on cross entropy, we define it as:\n",
    "\n",
    "$$ H(P,Q) =  -\\sum_{x \\in X} P(x) \\cdot \\log Q(x) $$\n",
    "\n",
    "Another metric (besides perplexity and cross-entropy) to compare two probability distributions is the Kullback-Leibler Divergence $D_{KL}$. It is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "D_{KL}(P\\|Q) = \\sum_{x \\in X}P(x) \\cdot \\log \\frac{P(x)}{Q(x)}\n",
    "\\end{equation}\n",
    "\n",
    "Where $P$ is the empirical or observed distribution, and Q is the estimated distribution over a common probabilitiy space $X$.\n",
    "\n",
    "As already explained, we use these two metrics to minimize the difference between an observed distribution and an estimation of the distribution. This will never be perfect, but we aim to minimize with the available information and resources.\n",
    "\n",
    "Answer the following questions regarding these two metrics:\n",
    "\n",
    "1. In the context of language modeling, briefly explain what $P$ and $Q$ are in the above expressions. Also, explain how can we compute cross-entropy in practice. (2 point)\n",
    "1. In the minimization problem mentioned above, i.e. minimizing the difference between the two distributions, is minimizing the cross-entropy, i.e. $ H(P,Q) $, and $D_{KL}(P\\|Q)$ equivalent? Support your answer with a mathematical expression. (2 point)\n",
    "1. In the lecture, it was mentioned that KL-Divergence is not a distance metric. Why is this? Provide a counter-example for the properties of a distance metric. (1 point)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Answer 1\n",
    "1. In the context of language modeling, Q would be the probability of a word from the distribution of the language model and P would be the probability of a word occurring in empirical data. In practice we would need a sufficiently large test corpus to calculate the distribution for P, while the distribution for Q is the distribution of the langauge model we are evaluating.\n",
    "2. In the context of minimizing the difference between empirical and estimated distributions (P and Q, respectively) the cross-entropy $H(P,Q)$ is equivalent to the KL-divergence $D_{KL}(P\\|Q)$, since the distribution over the observed data remains constant.\n",
    "    a. The entropy of the observed data is given by: $H(P)=-\\sum_{x \\in X}  P(x)\\cdot log P(x)$</br>\n",
    "    b. The cross-entropy between the observed data and the estimated distribution, from above: $ H(P,Q) =  -\\sum_{x \\in X} P(x) \\cdot \\log Q(x) $</br>\n",
    "    c. The KL-divergence can be decomposed in terms of $P$ and $Q$: </br>\n",
    "    $D_{KL}(P\\|Q) = \\sum_{x \\in X}P(x) \\cdot \\log \\frac{P(x)}{Q(x)}$</br>\n",
    "    $D_{KL}(P\\|Q) = \\sum_{x \\in X}P(x) \\cdot (\\log P(x)-\\log Q(x))$</br>\n",
    "    $D_{KL}(P\\|Q) = \\sum_{x \\in X}P(x) \\cdot \\log P(x)-\\sum_{x \\in X}P(x) \\cdot \\log Q(x)$</br>\n",
    "    $D_{KL}(P\\|Q) = -H(P) + H(P,Q)$</br>\n",
    "    d. Since the entropy of the empirical distribution $H(P)$ remains constant, minimizing cross-entropy $H(P,Q)$ is functionally equivalent to minimizing the KL-divergence $D_{KL}(P\\|Q)$. Put another way, the difference between $H(P,Q)$ and $D_{KL}(P\\|Q)$ is just the entropy of the empirical distribution, which for a given dataset remains constant.\n",
    "3. KL-Divergence is not a distance metric since distance metrics must be symmetric in both directions of a given relationship, and in practice KL-Divergence is not symmetric. That is, $D_{KL}(P\\|Q) \\neq D_{KL}(Q\\|P)$</br>\n",
    "Given the definition of KL-Divergence, unless $P(x) \\equiv Q(x)$, it is easy to see that</br>\n",
    "$$\n",
    "\\sum_{x \\in X}P(x) \\cdot \\log \\frac{P(x)}{Q(x)} \\neq \\sum_{x \\in X}Q(x) \\cdot \\log \\frac{Q(x)}{P(x)}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 2 - Prefix coding (5 points)\n",
    "\n",
    "As explained in the lecture, a nice way of constructing a code would be, is to determine the length of the encoding a token based on the frequency of the token. This can be done in many ways. In the lecture we talked about prefix codes:\n",
    "\n",
    "* No code word is a prefix of another code word\n",
    "* We can organize the code as a tree\n",
    "\n",
    "1. Given an arbitrary alphabet along with probabilities for each token, you are to implement a function that outputs the **Shannon-Fano encoding** for each character. (3 points.)\n",
    "\n",
    "Hint: feel free to use the example in the slides to validate that your generated encoding is correct:\n",
    "\n",
    "| word | frequency | $C(\\text{word})$ |\n",
    "| ---- | --------- | ------- |\n",
    "|\"the\" | 0.5       | `0`     |\n",
    "|\"and\" | 0.25      | `10`    |\n",
    "|\"of\"  | 0.125     | `110`   |\n",
    "|\"he\"  | 0.125     | `111`   |\n",
    "\n",
    "Where $C(\\text{word})$ represents the encoding of `word`.\n",
    "\n",
    "Though this algorithm is generalizable to any base of the code (i.e. the code need not be binary), we shall limit this exercise to binary encoding.\n",
    "\n",
    "2. The type of encoding from above does not always achieve the optimal encoding given an alphabet and its associated set of distributions. Provide an example for which this algorithm fails to find the optimal coding (1 point)\n",
    "3. An algorithm called Huffman coding exists that can achieve an optimal encoding in every case. Describe briefly how it differs from the simple prefix coding we used for this exercise. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T13:25:46.666710900Z",
     "start_time": "2023-05-18T13:25:46.520688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import functions from bonus file\n",
    "from importlib import reload\n",
    "import exercise_2\n",
    "exercise_2 = reload(exercise_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T13:25:46.999883700Z",
     "start_time": "2023-05-18T13:25:46.956966800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'the': '0', 'and': '10', 'of': '110', 'he': '111'}"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test case. Feel free to add more.\n",
    "encoding = exercise_2.get_encoding({'the': 0.5, 'and': 0.25, 'of': 0.125, 'he': 0.125})\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "{'A': '00', 'B': '01', 'C': '10', 'D': '110', 'E': '111'}"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = exercise_2.get_encoding({'A': 0.385, 'B': 0.179, 'C': 0.154, 'D': 0.154, \"E\": 0.128})\n",
    "encoding"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T13:25:47.847517900Z",
     "start_time": "2023-05-18T13:25:47.657222400Z"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2\n",
    "2. Since Shannon-Fano encoding depends upon a heirarchical sorting by cumulative probabilites, some distributions will have multiple options where to split the groups, and depending on the outcome of the code word lengths, an encoding may then have suboptimal code word lengths.\n",
    "\n",
    "Let us compare two encodings:</br>\n",
    "First, we split our alphabet into group 0: {\"the\"} and group 1: {\"and\", \"of\", \"he\", \"she\"}:</br>\n",
    "| word | frequency | $C(\\text{word})$ |\n",
    "| ---- | --------- | ------- |\n",
    "|\"the\" | 0.3       | `0`     |\n",
    "|\"and\" | 0.3       | `100`   |\n",
    "|\"of\"  | 0.1       | `101`   | \n",
    "|\"he\"  | 0.1       | `110`   |\n",
    "|\"she\" | 0.1       | `111`   |\n",
    "\n",
    "This has an expected length per word is given by $L=\\sum_i l_i\\cdot p(w_i)$</br>\n",
    "$L = (0.3*1) + (0.3*3) + 3*(0.1*3) = 2.1$</br>\n",
    "\n",
    "If we had instead initially split our alphabet into group 0: {\"the\", \"and\"} and group 1: {\"of\", \"he\", \"she\"}:</br>\n",
    "| word | frequency | $C(\\text{word})$ |\n",
    "| ---- | --------- | ------- |\n",
    "|\"the\" | 0.3       | `00`    |\n",
    "|\"and\" | 0.3       | `01`    |\n",
    "|\"of\"  | 0.1       | `10`    | \n",
    "|\"he\"  | 0.1       | `110`   |\n",
    "|\"she\" | 0.1       | `111`   |\n",
    "\n",
    "The expected length would instead be:\n",
    "$L = 2*(0.3*2) + (0.1*2) + 2*(0.1*3) = 2$</br>\n",
    "\n",
    "3. The main difference between the Huffman encoding algorithm is that instead of a heirarchical division based on cumulative probabilies, the Huffman encoding assigns each word in the alphabet a binary encoding based on a top-down binary tree, meaning the highest frequency words will have the shortest codes, as shown below.\n",
    "\n",
    "| word | frequency | $C(\\text{word})$ |\n",
    "| ---- | --------- | ------- |\n",
    "|\"the\" | 0.3       | `11`    |\n",
    "|\"and\" | 0.3       | `10`    |\n",
    "|\"of\"  | 0.1       | `00`    | \n",
    "|\"he\"  | 0.1       | `011`   |\n",
    "|\"she\" | 0.1       | `010`   | \n",
    "\n",
    "This is the same encoding as one of the two solutions found by the Shannon-Fano algorithm, specifically the optimal encoding between the two."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus - Long range dependencies (2 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we saw how being able to handle long-range dependencies can be valuable. Not only that, but we also saw some tokens also rely on medium- or short-range dependencies.\n",
    "\n",
    "Modern models are quite good at capturing all of these dependencies. Or are they?! In this exercise we will be going over some simple examples and evaluating how modern language models can handle long-range dependencies.\n",
    "\n",
    "NOTE: For this task you will probably have to **rely on Google Colab in order to run the relevant code**.\n",
    "\n",
    "We will be using a light-weight (~120M parameters), but relatively modern language model: GPT-2. GPT-2 is a generative model which can is conditioned on its input in order to generate new tokens. In future lectures we will learn more about language modelling and how this model can be trained. For now we will only be using it very practically.\n",
    "\n",
    "This model will give us a distribution of probabilities over the tokens in the vocabulary of the model which indicate what it thinks the next word should be conditioned on the input you give it.\n",
    "\n",
    "Assuming a sequence $(w_1, w_2, \\dots, w_n)$, the model outputs a distribution for which this holds true:\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^N p(w_j|w_1, w_2, \\dots, w_{n-1}) = 1\n",
    "$$\n",
    "\n",
    "Where $N$ is the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries.\n",
    "# NOTE: Remember, run this on Google Colab (unless you know what you're doing)\n",
    "!pip install -q torch huggingface_hub tokenizers sentencepiece sacremoses importlib_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from bonus file\n",
    "from importlib import reload\n",
    "import bonus\n",
    "bonus = reload(bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\hub.py:267: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/huggingface/transformers/zipball/main\" to C:\\Users\\William/.cache\\torch\\hub\\main.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b019c179154a2f9443c2efcda2ee9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\William\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949fa13631504b78b1157553680a353e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b960fd8406704f22a605785e8f2bd000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\hub.py:267: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/huggingface/pytorch-transformers/zipball/main\" to C:\\Users\\William/.cache\\torch\\hub\\main.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b52c2283ec142afb8477170b4a9bcd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bbadae65c04ecfa453c0fc8bf45ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f7bffda69041ba892d2673c0aac7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Download model and tokenizer from huggingface.co or cache\n",
    "model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', 'gpt2')\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'gpt2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A familiar example\n",
    "\n",
    "We have provided you with the code to run inference on the model and be able to evaluate its output. However, **feel free to change the code if you are not satisfied with how it is set up initially**.\n",
    "\n",
    "In the lecture we discussed a repeating sequence where entropy goes to zero after a certain length is reached. How does that look in modern models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0.42868295311927795, 'D': 0.14357711374759674, 'C': 0.13164374232292175, 'B': 0.08625605702400208, 'E': 0.012573751620948315}\n",
      "A B C A B C A\n"
     ]
    }
   ],
   "source": [
    "bonus.model_predict('A B C A B C', model, tokenizer, top_picks=5)\n",
    "\n",
    "# Expected output:\n",
    "# {'A': 0.42868340015411377, 'D': 0.14357835054397583, 'C': 0.13164187967777252, 'B': 0.08625581860542297, 'E': 0.01257376465946436}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model_predict(\u001B[39m'\u001B[39m\u001B[39mA B C A B C A B C\u001B[39m\u001B[39m'\u001B[39m, model, tokenizer, \u001B[39m5\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[39m# Expected output:\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[39m# {'A': 0.7761804461479187, 'C': 0.07017528265714645, 'B': 0.04758051782846451, 'D': 0.01468430832028389, 'E': 0.003714686958119273}\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model_predict' is not defined"
     ]
    }
   ],
   "source": [
    "model_predict('A B C A B C A B C', model, tokenizer, 5)\n",
    "\n",
    "# Expected output:\n",
    "# {'A': 0.7761804461479187, 'C': 0.07017528265714645, 'B': 0.04758051782846451, 'D': 0.01468430832028389, 'E': 0.003714686958119273}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the conditional entropy would be going down as the probability distribution gets more and more skewed towards `A` if we condition the model on a longer repeating sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your task\n",
    "\n",
    "Come up with sequences where you can evaluate how the model deals with long-range dependencies. How would you estimate the conditional entropy of the model as you modify the phrases? Present and explain **at least** 2 examples (1+1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus.model_predict('TODO: YOUR PHRASE HERE', model, tokenizer, 5)\n",
    "# TODO: Add more examples here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "venv",
   "language": "python",
   "display_name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
