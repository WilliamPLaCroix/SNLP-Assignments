{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "venv",
   "language": "python",
   "display_name": "venv"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SNLP Assignment 4 - The Plagues of N-Gram Modelling\n",
    "\n",
    "Name 1: <br/>\n",
    "Student id 1: <br/>\n",
    "Email 1: <br/>\n",
    "\n",
    "\n",
    "Name 2: <br/>\n",
    "Student id 2: <br/>\n",
    "Email 2: <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the respective Python files for any additional exercises necessary. There is no need to submit the data files should they exist. <br/>\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2.zip**. Make sure to click on \"Turn-in\" (or the equivalent on CMS) after your upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "CglCcCF6kXDi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Plague of OOVs (5)\n",
    "\n",
    "Out-Of-Vocabulary words or OOVs are a major problem for any language model because of insufficent data and/or the emergence of new words. Let's see how this affects our statistical models. \n",
    "\n",
    "1. What happpens to the perplexity when there is an OOV in the evaluation sentence? (0.5 points)\n",
    "\n",
    "\n",
    "2. The go-to solution for modelling OOVs in the N-gram setting is to introduce a new `<unk>` token in the vocabulary for all unknown words. The `<unk>` token replaces all OOVs and is then modelled like any other word. (4 points)\n",
    "    - Split your data into train:test datasets using a 70:30 ratio. (0.5 points) \n",
    "    - Complete the function to create a vocabulary with the *top_n* most frequent words in the train set. (0.5 points)\n",
    "    - Complete the function that restricts a corpus into the given vocabulary. (1 point)\n",
    "    -  Vary *top_n* and plot how the OOV rate for the test set changes with an increase in the size of the vocabulary (use a log-log scale). What do you observe? (2 points)\n",
    "  \n",
    "\n",
    "3. A very common practice is to build the vocabulary using all words that occur twice or more in the training data. Why would we restict the vocabulary if OOVs are a headache in the first place? (0.5 points)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "NzS2w_A6kXgI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "import oov\n",
    "from importlib import reload\n",
    "oov=reload(oov)\n",
    "import numpy as np\n",
    "\n",
    "#Loading the WSJ treebank, Implement preprocessing\n",
    "corpus=oov.load_and_preprocess_data()\n",
    "\n",
    "#Implement corpus splitting. Do not randomize anything here.\n",
    "train,test=oov.train_test_split(corpus)\n",
    "\n",
    "for top_n in np.linspace(100,5000,10,dtype=int):\n",
    "  # Create Vocabulary with most popular words in the train set\n",
    "  vocab=oov.make_vocab(train,top_n)\n",
    "\n",
    "\n",
    "  # Force the train data and test data into this vocabulary by replacement with '<unk>'\n",
    "  vocabulary_restricted_train=oov.restrict_vocab(train,vocab)\n",
    "  vocabulary_restricted_test=oov.restrict_vocab(test,vocab)\n",
    "\n",
    "  # TODO: Find and plot the oov rate. Use loglog axes. \n"
   ],
   "metadata": {
    "id": "-VdNEPSFkolX",
    "ExecuteTime": {
     "end_time": "2023-05-24T14:42:57.264218Z",
     "start_time": "2023-05-24T14:42:54.190867100Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Nicho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 19\u001B[0m\n\u001B[0;32m     15\u001B[0m vocab\u001B[38;5;241m=\u001B[39moov\u001B[38;5;241m.\u001B[39mmake_vocab(train,top_n)\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Force the train data and test data into this vocabulary by replacement with '<unk>'\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m vocabulary_restricted_train\u001B[38;5;241m=\u001B[39m\u001B[43moov\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrestrict_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43mvocab\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m vocabulary_restricted_test\u001B[38;5;241m=\u001B[39moov\u001B[38;5;241m.\u001B[39mrestrict_vocab(test,vocab)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# TODO: Find and plot the oov rate. Use loglog axes. \u001B[39;00m\n",
      "File \u001B[1;32mD:\\snlp\\SNLP-Assignments\\Assignment_4\\oov.py:61\u001B[0m, in \u001B[0;36mrestrict_vocab\u001B[1;34m(corpus, vocab)\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrestrict_vocab\u001B[39m(corpus, vocab):\n\u001B[0;32m     57\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''Make the corpus fit inside the vocabulary using <unk>\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;124;03m  Input: corpus - List[List[str]]\u001B[39;00m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;124;03m         vocab  - List[str]\u001B[39;00m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;124;03m  Output: Vocabulary_restricted_corpus - List[List[str]]'''\u001B[39;00m\n\u001B[1;32m---> 61\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Plague of Unseen N-Grams (5 points)\n",
    "\n",
    "A major and very common issue with N-gram modelling is the estimation of probabilities for ngram sequences that are unobserved in the training data. The most popular technique to tackle unseen ngrams is to smooth the MLE distribution. We will deal with more complicated smoothing techniques later in the course, but let's look at a rudimentary smoothing technique called Laplace Smoothing. You can look it up in the [Jurafsky Book](https://web.stanford.edu/~jurafsky/slp3/old_dec21/3.pdf) \n",
    "\n",
    "The idea of Laplace smoothing is simple: You add a count of alpha to all existing bigram counts in the corpus. Consequently, you \"pretend\" that you observed the previously unseen ngrams **once**. For intuition, you can look at Figures 3.1 and 3.6 in the Jurafsky book. Your task now is to implement a Bigram model with add-one smoothing. \n",
    "\n",
    "1. The main task is to complete the `BigramModel` class in `smoothed_lm.py`. Finish the functions to count ngrams and compute the Laplace smoothed probability for a given bigram. (2 points)\n",
    "\n",
    "2. Now calculate the average perplexity on the test set. You can start conditional probabilitity estimations from the second word (pay attention to the consequent normalization factor for perplexity). Use vocabulary-restricted train and test sets. Use a top_5000 vocabulary. (2 point).\n",
    "\n",
    "3. What happens when you vary alpha in the range 0-1? Why is this smoothing inefficient? (1 point)"
   ],
   "metadata": {
    "id": "IeOMNq7vkelL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import smoothed_lm \n",
    "from importlib import reload\n",
    "smoothed_lm=reload(smoothed_lm)\n",
    "\n",
    "# Create Vocabulary with most popular 5000 words in the train set\n",
    "vocab=oov.make_vocab(train,5000)\n",
    "\n",
    "# Force the train data and test data into the vocabulry\n",
    "vocabulary_restricted_train=oov.restrict_vocab(train,vocab)\n",
    "vocabulary_restricted_test=oov.restrict_vocab(test,vocab)\n",
    "\n",
    "\n",
    "#Complete the class\n",
    "model=smoothed_lm.BigramModel(vocabulary_restricted_train,vocabulary_restricted_test,alpha=0.5)\n",
    "\n",
    "#Calculate Average test perplexity\n",
    "model.perplexity()"
   ],
   "metadata": {
    "id": "Ep0lhFg2kup0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bonus Question - Alternate tokenizations for OOV handling (2 points)\n",
    "\n",
    "We saw in the first part that the OOV issue can be mitigated by using the `<unk>` token. Another option to deal with the OOV problem is to change your tokenization schema by making it more granular. A very popular method along these lines is subword modelling. The idea here is simple: you tokenize your sentence into a sequence of sub-words and subsequently span your vocabulary across all possible subwords in the corpus. This makes sure that there is no OOV in the test set, since new words are still composed of the subwords in your vocabulary. \n",
    "\n",
    "\n",
    "1. Tokenize the corpus using character-level tokenization and compute the perplexity of the resulting bigram model. (0.5 points)\n",
    "\n",
    "2. Extend this to another tokenization scheme: The very popular [Byte-Pair Encoding](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt) schema used these days. You can use Huggingface's [GPT2Tokenizer](https://huggingface.co/transformers/v3.0.2/model_doc/gpt2.html#gpt2tokenizer) to do this.  Find the resulting bigram model perplexity. (0.5 points)\n",
    "\n",
    "3. Can you compare these perplexities? In general, is it okay to compare perplexities when we use different tokenization schemes? (1 point)\n"
   ],
   "metadata": {
    "id": "5I-CCLXwkiLJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDhRg3MumGIr",
    "outputId": "5c18de18-5a43-45a7-b21f-723868aaf7c7",
    "ExecuteTime": {
     "end_time": "2023-05-24T13:10:37.422146400Z",
     "start_time": "2023-05-24T13:10:34.704934Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\snlp\\venv\\lib\\site-packages (4.29.2)\n",
      "Requirement already satisfied: filelock in d:\\snlp\\venv\\lib\\site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in d:\\snlp\\venv\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\snlp\\venv\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\snlp\\venv\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\snlp\\venv\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\snlp\\venv\\lib\\site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in d:\\snlp\\venv\\lib\\site-packages (from transformers) (2.30.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\snlp\\venv\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\snlp\\venv\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in d:\\snlp\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\snlp\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in d:\\snlp\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\snlp\\venv\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\snlp\\venv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\snlp\\venv\\lib\\site-packages (from requests->transformers) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\snlp\\venv\\lib\\site-packages (from requests->transformers) (2023.5.7)\n"
     ]
    }
   ]
  }
 ]
}
