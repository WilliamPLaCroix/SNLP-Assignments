{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SNLP Assignment 5 \n",
        "\n",
        "Name 1: <br/>\n",
        "Student id 1: <br/>\n",
        "Email 1: <br/>\n",
        "\n",
        "\n",
        "Name 2: <br/>\n",
        "Student id 2: <br/>\n",
        "Email 2: <br/> \n",
        "\n",
        "**Instructions:** Read each question carefully. <br/>\n",
        "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the respective Python files for any additional exercises necessary. There is no need to submit the data files should they exist. <br/>\n",
        "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2.zip**. Make sure to click on \"Turn-in\" (or the equivalent on CMS) after your upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "kLJnh3mUN_O_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Back-Off and Interpolation (5 points) </br>\n",
        "\n",
        "In Assignment 4, we implemented a simple smoothing technique by presuming counts where they did not exist. A more informed version of smoothing is to look into lower order n-grams when we see novel word combinations. There are two popular ways of doing this: Katz backoff and linear interpolation. \n",
        "\n",
        "\n",
        "1. Look these terms up and describe how they differ in 3-4 sentences (0.5 points)\n",
        "\n",
        "2. We generally use cross validation to determine the hyperparameters used in these models. Describe k-fold cross validation in your own words (0.5 points)\n",
        "\n",
        "3. Let's implement an interpolated ngram model. Write a Language model class that trains an ngram model on a training corpus ```(List[List[str]])``` and computes the perplexity on a test corpus ```(List[List[str]])```. Use add-$\\alpha$ smoothing and interpolation weights of $\\lambda_i=\\frac{1}{n}, \\forall i \\in \\{1\\dots n\\}$ where $n$ is the order of the model. As usual, you will need to account for OOV tokens with ```<unk>```. Use the top_5000 words for vocabulary. You may reuse code from Asignment 4. (2 points)\n",
        "\n",
        "4. Find the optimal $\\alpha$ for smoothing and the optimal n-gram order using 5-fold cross validation (independently).  (2 points)\n",
        "\n",
        "We expect you to make some design choices yourself. Please document them appropriately."
      ],
      "metadata": {
        "id": "eDvPIUircPZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import exercise_1\n",
        "from importlib import reload\n",
        "exercise_1=reload(exercise_1)\n",
        "\n",
        "corpus=exercise_1.load_and_preprocess_data()\n",
        "\n",
        "#TODO: use a 70:30 split for train/test. Take test from the last 30%. Do not randomize at this stage.\n",
        "#train,test=split_corpus(corpus)\n",
        "\n",
        "\n",
        "#TODO: Finish this model. Pass necessary arguments\n",
        "#model=exercise_1.Interpolated_Model(args)\n",
        "\n",
        "#Calculate test perplexity\n",
        "#Interpolated_model.perplexity()\n",
        "\n",
        "#implement cross validation to find optimal n and alpha. You might want to do this independently.  \n",
        "#Plot perplexity vs alpha graph.\n",
        "#Plot perplexity vs n graph. \n",
        "#Write up conclusions. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTk6N86uqT1m",
        "outputId": "78179d4c-d0e0-484f-e4c4-b7a0a52ff3f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Kneser-Ney Smoothing (5 points)\n",
        "\n",
        "\n",
        "Consider the following notation\n",
        "\n",
        "\n",
        "- $V$ - LM vocabulary\n",
        "-  $N (x)$ - count of the n-gram x in the training corpus \n",
        "- $N_+(\\bullet w) \\triangleq |\\{u : N (u, w) > 0\\}|$ - number of bigram types ending in w\n",
        "- $N_+(w \\bullet) \\triangleq |\\{u : N (w, u) > 0\\}|$ - number of bigram types starting with w\n",
        "- $N_+(\\bullet w \\bullet) \\triangleq |\\{(u,v) : N (u, w, u) > 0\\}|$ - number of trigram types with w in the middle\n",
        "\n",
        "This exercise aims to provide a basic understanding of Kneser-Ney Smoothing. Kneser-Ney Smoothing makes use of *continuation counts* of words for lower order n-grams, given as\n",
        "\n",
        "\\begin{equation}\n",
        "C_{KN} = \n",
        "\\begin{cases}\n",
        "\\text{count}(\\bullet) & \\text{for highest order} \\\\\n",
        "\\text{continuationcount}(\\bullet) & \\text{for lower orders}\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "For a trigram model, Kneser-Ney Smoothing is implemented using the following equations:\n",
        "\n",
        "$$P_{KN}(w_3|w_1, w_2) = \\frac{\\max\\{N(w_1 w_2 w_3)-d,0\\}}{N(w_1 w_2)} + \\lambda(w_1, w_2)P_{KN}(w_3|w_2)$$\n",
        "\n",
        "$$P_{KN}(w_3|w_2) = \\frac{\\max\\{N_{+}(\\bullet w_2 w_3)-d,0\\}}{N_{+}(\\bullet w_2 \\bullet)} + \\lambda(w_2)P_{KN}(w_3)$$\n",
        "\n",
        "\\begin{equation}\n",
        "P_{KN}(w_3) = \\begin{cases}\n",
        "\\frac{N_{+}(\\bullet w_3)}{N_{+}(\\bullet \\bullet)} & \\text{if $w_3 \\in$ V} \\\\\n",
        "\\frac{1}{V} & \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "$\\lambda$ is used to normalise the discounted probability mass and is given by\n",
        "\n",
        "$$\\lambda(w_1, w_2) = \\frac{d}{N(w_1 w_2)} \\cdot N_{+}(w_1 w_2 \\bullet)$$\n",
        "\n",
        "$$\\lambda(w_2) = \\frac{d}{N(w_2)} \\cdot N_{+}(w_2 \\bullet)$$\n",
        "\n",
        "\n",
        "1. Explain the main intuition behind Kneser-Ney language models and discuss how it is\n",
        "different from absolute discounting LMs [1 point]\n",
        "\n",
        "2. Assume that we make a trigram model from [alice_in_wonderland.txt](https://gist.github.com/phillipj/4944029). Preprocess the text by first removing all punctuation, lowercasing, and then tokenisation. There is no need to split the data into train and test sets. (0.5 points)\n",
        "\n",
        "3. Code a function that calculates the bigram probability $P_{KN}(w_3|w_2)$ using the equations provided. The discounting parameter d = 0.75. Use your function to calculate the probabilities $P_{KN}(nothing|said)$ and $P_{KN}(nichts|said)$. (2 points)\n",
        "\n",
        "4. Manually compute the MLE and the Add-1 probabilities for the same bigrams. How do these compare to Kneser-Ney smoothing? (1 point)"
      ],
      "metadata": {
        "id": "BMTXgBYllGVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib import reload\n",
        "import exercise_2\n",
        "exercise_2=reload(exercise_2)\n",
        "\n",
        "text=open('alice_in_wonderland.txt').read()\n",
        "tokenized_text=exercise_2.perprocess_text(text)\n",
        "\n",
        "#Feel free to not use a class. This is just boilerplate.\n",
        "#BigramCounter=exercise_2.KneserNeyCounter(args)\n",
        "#BigramCounter.prob(bigram)"
      ],
      "metadata": {
        "id": "qKCweesLqY0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus Question: Good Turing Smoothing (2 points)"
      ],
      "metadata": {
        "id": "cJWA44qR5k8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We know that Add-alpha discounting is not very efficient since it takes away too much probability mass for distribution. A better alternative is Good-Turing smoothing, whoch distributes probbaility mass for a lower order ngram from immediately higher ngrams. \n",
        "\n",
        "1. Describe the central idea behind Good-Turing smoothing and state the formulas for Good Turing counts and Good Turing probability. (0.5)\n",
        "\n",
        "2. Given some fixed history, How much probability mass is set aside when we use Good Turing counts? Express this mathematically. (1 point) \n",
        "\n",
        "   \n",
        "3. What is the main problem with Good Turing estimation? (0.5)\n"
      ],
      "metadata": {
        "id": "BIr09Uu1lKUw"
      }
    }
  ]
}