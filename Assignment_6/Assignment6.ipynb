{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLP Assignment 6 - Text Classification\n",
    "\n",
    "Name 1: William LaCroix<br/>\n",
    "Student id 1: 7038732<br/>\n",
    "Email 1: williamplacroix@gmail.com<br/>\n",
    "\n",
    "\n",
    "Name 2: Nicholas Jennings<br/>\n",
    "Student id 2: 2573492<br/>\n",
    "Email 2: s8nijenn@stud.uni-saarland.de<br/>\n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the respective Python files for any additional exercises necessary. There is no need to submit the data files should they exist. <br/>\n",
    "Upload the zipped folder on CMS. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Sentiment Analysis</span>\n",
    "\n",
    "   - Use the [FinancialPhraseBank](https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news) corpus with a 80:20 train test split, with a) a naive bayes b) XGBoost classifier for sentiment analysis (**2 points**)\n",
    "\n",
    "      - Naive Bayes: https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "      - XGBoost: https://xgboost.readthedocs.io/en/stable/index.html\n",
    "\n",
    "      - <span style=\"color:red\">Note:</span> Only use all_data.csv file for your experiments\n",
    "\n",
    "   - Use [LoughranMcDonald master dictionary](https://drive.google.com/file/d/17CmUZM9hGUdGYjCXcjQLyybjTrcjrhik/view?usp=sharing) to get word level polarity, replace words with the their corresponding polarity and now repeat the same classification task as above (**4 points**)\n",
    "\n",
    "      - What happens if you remove the stop words? \n",
    "\n",
    "      - What's the ratio of stopwords (taken from NLTK) amongst the total word count? \n",
    "\n",
    "      - Is it a good choice to do stopword removal? Explain with 2-3 examples why or why not?\n",
    "   \n",
    "   - Create cascaded Bi-Grams and tri-grams and repeat both the exercises from above. Which representation do you think is a better value in terms of accuracy and computational power required? (**3 points**)\n",
    "\n",
    "      - Bi-grams example: \n",
    "\n",
    "         - Company A barely surpassed their profit expectations.\n",
    "         \n",
    "         - (Company, A), (A, barely), (barely, surpassed), (surpassed, their) ...\n",
    "   \n",
    "   - How would you represnt the polarity in uni/bi/tri-gram models with Huffman Encoding? (**1 point**)\n",
    "\n",
    "\n",
    "### Few points to remember\n",
    "   - While splitting your dataset, use seed=42\n",
    "   - Use type hint(s) in your code, and add a docstring to your functions or classes\n",
    "   - Focus on the readability of your code, that helps us to give you better feedback on where the code went wrong\n",
    "   - <span style=\"color:red\">Do not submit the data or dictionary file.</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "\n",
    "- What happens if you remove the stop words?\n",
    "    - Answer:\n",
    "\n",
    "- What's the ratio of stopwords (taken from NLTK) amongst the total word count? \n",
    "    - Answer:\n",
    "\n",
    "- Is it a good choice to do stopword removal? Explain with 2-3 examples why or why not?\n",
    "    - Answer:\n",
    "  \n",
    "- How would you represnt the polarity in uni/bi/tri-gram models with Huffman Encoding? (**1 point**)\n",
    "    - Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import solution\n",
    "from importlib import reload\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  negative  neutral  positive  All\n",
      "True                                       \n",
      "negative         53       33        24  110\n",
      "neutral          11      502        58  571\n",
      "positive         10      115       164  289\n",
      "All              74      650       246  970\n",
      "\n",
      "Baseline unpolairzed unigram Naive Bayes accuracy: 0.7412371134020619\n",
      "Baseline unpolairzed unigram Naive Bayes precision: 0.5674740484429066\n",
      "Baseline unpolairzed unigram Naive Bayes recall: 0.7420814479638009\n",
      "Baseline unpolairzed unigram Naive Bayes F1: 0.6431372549019608\n"
     ]
    }
   ],
   "source": [
    "# 1. Unigram Naive Bayes, unpolarized -> baseline NB\n",
    "solution = reload(solution)\n",
    "classification_model = MultinomialNB()\n",
    "corpus = solution.load_and_preprocess_data()\n",
    "\n",
    "y_test, y_pred = solution.train_and_fit_model(corpus, classification_model)\n",
    "\n",
    "# Get the confusion matrix from solution.py file, since it's a multi-class classification\n",
    "# Confusion matrix should give you a better idea of how well your model is performing\n",
    "# Naive Bayes classification of 3 class labels\n",
    "# NB_confusion_matrix on 3x3 matrix\n",
    "confusion_matrix = solution.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "solution.test(confusion_matrix, classifier=\"Naive Bayes\", preprocessing=\"Baseline unpolairzed unigram\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  negative  neutral  positive  All\n",
      "True                                       \n",
      "negative         71       31         8  110\n",
      "neutral          11      541        19  571\n",
      "positive          4      123       162  289\n",
      "All              86      695       189  970\n",
      "\n",
      "Baseline unpolairzed unigram XGBoost accuracy: 0.797938144329897\n",
      "Baseline unpolairzed unigram XGBoost precision: 0.5605536332179931\n",
      "Baseline unpolairzed unigram XGBoost recall: 0.8059701492537313\n",
      "Baseline unpolairzed unigram XGBoost F1: 0.6612244897959184\n"
     ]
    }
   ],
   "source": [
    "# 2. Unigram XGBoost, unpolarized -> baseline XGBoost\n",
    "solution = reload(solution)\n",
    "classification_model = XGBClassifier()\n",
    "corpus = solution.load_and_preprocess_data()\n",
    "\n",
    "y_test, y_pred = solution.train_and_fit_model(corpus, classification_model)\n",
    "\n",
    "# Get the confusion matrix from solution.py file\n",
    "confusion_matrix = solution.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "solution.test(confusion_matrix, classifier=\"XGBoost\", preprocessing=\"Baseline unpolairzed unigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0                                                  1\n",
      "0     1  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...\n",
      "1     1  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...\n",
      "2     0  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...\n",
      "3     2  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...\n",
      "4     2  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...\n",
      "...  ..                                                ...\n",
      "4841  0  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...\n",
      "4842  1  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...\n",
      "4843  0  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...\n",
      "4844  0  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...\n",
      "4845  0  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...\n",
      "\n",
      "[4846 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m corpus \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39mload_and_preprocess_data(polarize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(corpus)\n\u001b[1;32m----> 7\u001b[0m y_test, y_pred \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39;49mtrain_and_fit_model(corpus, classification_model)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Get the confusion matrix from solution.py file\u001b[39;00m\n\u001b[0;32m     10\u001b[0m confusion_matrix \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39mconfusion_matrix(y_test, y_pred)\n",
      "File \u001b[1;32mg:\\My Drive\\UdS\\Classes\\SNLP\\working\\solution.py:203\u001b[0m, in \u001b[0;36mtrain_and_fit_model\u001b[1;34m(corpus, classification_model)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_and_fit_model\u001b[39m(corpus: \u001b[39m\"\u001b[39m\u001b[39mtuple[list[str|int], list[str]]\u001b[39m\u001b[39m\"\u001b[39m, classification_model) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtuple[list[int], list[int]]\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    193\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m    Trains and fits a classification model using a given corpus.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39m        Tuple[List[int], List[int]]: A tuple containing the true labels and predicted labels for the test dataset.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m     corpus_features \u001b[39m=\u001b[39m vectorize_set(corpus[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m    204\u001b[0m     corpus_labels \u001b[39m=\u001b[39m corpus[\u001b[39m0\u001b[39m]\n\u001b[0;32m    205\u001b[0m     X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(corpus_features, corpus_labels, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "File \u001b[1;32mg:\\My Drive\\UdS\\Classes\\SNLP\\working\\solution.py:221\u001b[0m, in \u001b[0;36mvectorize_set\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39m# encode sentences as one-hot vectors using scikit-learn's one_hot function OHE\u001b[39;00m\n\u001b[0;32m    220\u001b[0m vectorizer \u001b[39m=\u001b[39m sklfe\u001b[39m.\u001b[39mCountVectorizer()\n\u001b[1;32m--> 221\u001b[0m \u001b[39mreturn\u001b[39;00m vectorizer\u001b[39m.\u001b[39;49mfit_transform(corpus)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         )\n\u001b[0;32m   1298\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# 3. Unigram Naive Bayes, polarized, include stop words\n",
    "solution = reload(solution)\n",
    "classification_model = MultinomialNB()\n",
    "corpus = solution.load_and_preprocess_data(polarize=True)\n",
    "\n",
    "print(corpus)\n",
    "y_test, y_pred = solution.train_and_fit_model(corpus, classification_model)\n",
    "\n",
    "# Get the confusion matrix from solution.py file\n",
    "confusion_matrix = solution.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "solution.test(confusion_matrix, classifier=\"Naive Bayes\", preprocessing=\"Polairzed unigram with stop words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m classification_model \u001b[39m=\u001b[39m MultinomialNB()\n\u001b[0;32m      4\u001b[0m corpus \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39mload_and_preprocess_data(remove_stops\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, polarize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m y_test, y_pred \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39;49mtrain_and_fit_model(corpus, classification_model)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Get the confusion matrix from solution.py file\u001b[39;00m\n\u001b[0;32m      9\u001b[0m confusion_matrix \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39mconfusion_matrix(y_test, y_pred)\n",
      "File \u001b[1;32mg:\\My Drive\\UdS\\Classes\\SNLP\\working\\solution.py:203\u001b[0m, in \u001b[0;36mtrain_and_fit_model\u001b[1;34m(corpus, classification_model)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_and_fit_model\u001b[39m(corpus: \u001b[39m\"\u001b[39m\u001b[39mtuple[list[str|int], list[str]]\u001b[39m\u001b[39m\"\u001b[39m, classification_model) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtuple[list[int], list[int]]\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    193\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m    Trains and fits a classification model using a given corpus.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39m        Tuple[List[int], List[int]]: A tuple containing the true labels and predicted labels for the test dataset.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m     corpus_features \u001b[39m=\u001b[39m vectorize_set(corpus[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m    204\u001b[0m     corpus_labels \u001b[39m=\u001b[39m corpus[\u001b[39m0\u001b[39m]\n\u001b[0;32m    205\u001b[0m     X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(corpus_features, corpus_labels, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "File \u001b[1;32mg:\\My Drive\\UdS\\Classes\\SNLP\\working\\solution.py:221\u001b[0m, in \u001b[0;36mvectorize_set\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39m# encode sentences as one-hot vectors using scikit-learn's one_hot function OHE\u001b[39;00m\n\u001b[0;32m    220\u001b[0m vectorizer \u001b[39m=\u001b[39m sklfe\u001b[39m.\u001b[39mCountVectorizer()\n\u001b[1;32m--> 221\u001b[0m \u001b[39mreturn\u001b[39;00m vectorizer\u001b[39m.\u001b[39;49mfit_transform(corpus)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         )\n\u001b[0;32m   1298\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# 4. Unigram Naive Bayes, polarized, exclude stop words\n",
    "solution = reload(solution)\n",
    "classification_model = MultinomialNB()\n",
    "corpus = solution.load_and_preprocess_data(remove_stops=True, polarize=True)\n",
    "\n",
    "y_test, y_pred = solution.train_and_fit_model(corpus, classification_model)\n",
    "\n",
    "# Get the confusion matrix from solution.py file\n",
    "confusion_matrix = solution.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "solution.test(confusion_matrix, classifier=\"Naive Bayes\", preprocessing=\"Polairzed unigram without stop words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m classification_model \u001b[39m=\u001b[39m XGBClassifier()\n\u001b[0;32m      4\u001b[0m corpus \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39mload_and_preprocess_data(polarize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m y_test, y_pred \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39;49mtrain_and_fit_model(corpus, classification_model)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Get the confusion matrix from solution.py file\u001b[39;00m\n\u001b[0;32m      9\u001b[0m confusion_matrix \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39mconfusion_matrix(y_test, y_pred)\n",
      "File \u001b[1;32mg:\\My Drive\\UdS\\Classes\\SNLP\\working\\solution.py:203\u001b[0m, in \u001b[0;36mtrain_and_fit_model\u001b[1;34m(corpus, classification_model)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_and_fit_model\u001b[39m(corpus: \u001b[39m\"\u001b[39m\u001b[39mtuple[list[str|int], list[str]]\u001b[39m\u001b[39m\"\u001b[39m, classification_model) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtuple[list[int], list[int]]\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    193\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m    Trains and fits a classification model using a given corpus.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39m        Tuple[List[int], List[int]]: A tuple containing the true labels and predicted labels for the test dataset.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m     corpus_features \u001b[39m=\u001b[39m vectorize_set(corpus[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m    204\u001b[0m     corpus_labels \u001b[39m=\u001b[39m corpus[\u001b[39m0\u001b[39m]\n\u001b[0;32m    205\u001b[0m     X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(corpus_features, corpus_labels, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "File \u001b[1;32mg:\\My Drive\\UdS\\Classes\\SNLP\\working\\solution.py:221\u001b[0m, in \u001b[0;36mvectorize_set\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39m# encode sentences as one-hot vectors using scikit-learn's one_hot function OHE\u001b[39;00m\n\u001b[0;32m    220\u001b[0m vectorizer \u001b[39m=\u001b[39m sklfe\u001b[39m.\u001b[39mCountVectorizer()\n\u001b[1;32m--> 221\u001b[0m \u001b[39mreturn\u001b[39;00m vectorizer\u001b[39m.\u001b[39;49mfit_transform(corpus)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         )\n\u001b[0;32m   1298\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# 5. Unigram XGBoost, polarized, include stop words\n",
    "solution = reload(solution)\n",
    "classification_model = XGBClassifier()\n",
    "corpus = solution.load_and_preprocess_data(polarize=True)\n",
    "\n",
    "y_test, y_pred = solution.train_and_fit_model(corpus, classification_model)\n",
    "\n",
    "# Get the confusion matrix from solution.py file\n",
    "confusion_matrix = solution.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "solution.test(confusion_matrix, classifier=\"XGBoost\", preprocessing=\"Polairzed unigram with stop words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m classification_model \u001b[39m=\u001b[39m XGBClassifier()\n\u001b[0;32m      4\u001b[0m corpus \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39mload_and_preprocess_data(remove_stops\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, polarize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m y_test, y_pred \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39;49mtrain_and_fit_model(corpus, classification_model)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Get the confusion matrix from solution.py file\u001b[39;00m\n\u001b[0;32m      9\u001b[0m confusion_matrix \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39mconfusion_matrix(y_test, y_pred)\n",
      "File \u001b[1;32mg:\\My Drive\\UdS\\Classes\\SNLP\\working\\solution.py:203\u001b[0m, in \u001b[0;36mtrain_and_fit_model\u001b[1;34m(corpus, classification_model)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_and_fit_model\u001b[39m(corpus: \u001b[39m\"\u001b[39m\u001b[39mtuple[list[str|int], list[str]]\u001b[39m\u001b[39m\"\u001b[39m, classification_model) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtuple[list[int], list[int]]\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    193\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m    Trains and fits a classification model using a given corpus.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39m        Tuple[List[int], List[int]]: A tuple containing the true labels and predicted labels for the test dataset.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m     corpus_features \u001b[39m=\u001b[39m vectorize_set(corpus[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m    204\u001b[0m     corpus_labels \u001b[39m=\u001b[39m corpus[\u001b[39m0\u001b[39m]\n\u001b[0;32m    205\u001b[0m     X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(corpus_features, corpus_labels, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "File \u001b[1;32mg:\\My Drive\\UdS\\Classes\\SNLP\\working\\solution.py:221\u001b[0m, in \u001b[0;36mvectorize_set\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39m# encode sentences as one-hot vectors using scikit-learn's one_hot function OHE\u001b[39;00m\n\u001b[0;32m    220\u001b[0m vectorizer \u001b[39m=\u001b[39m sklfe\u001b[39m.\u001b[39mCountVectorizer()\n\u001b[1;32m--> 221\u001b[0m \u001b[39mreturn\u001b[39;00m vectorizer\u001b[39m.\u001b[39;49mfit_transform(corpus)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         )\n\u001b[0;32m   1298\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# 6. Unigram XGBoost, polarized, exclude stop words\n",
    "solution = reload(solution)\n",
    "classification_model = XGBClassifier()\n",
    "corpus = solution.load_and_preprocess_data(remove_stops=True, polarize=True)\n",
    "\n",
    "y_test, y_pred = solution.train_and_fit_model(corpus, classification_model)\n",
    "\n",
    "# Get the confusion matrix from solution.py file\n",
    "confusion_matrix = solution.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "solution.test(confusion_matrix, classifier=\"XGBoost\", preprocessing=\"Polairzed unigram without stop words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m classification_model \u001b[39m=\u001b[39m MultinomialNB()\n\u001b[0;32m      4\u001b[0m corpus \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39mload_and_preprocess_data(polarize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, ngramize\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m y_test, y_pred \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39;49mtrain_and_fit_model(corpus, classification_model)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Get the confusion matrix from solution.py file\u001b[39;00m\n\u001b[0;32m      9\u001b[0m confusion_matrix \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39mconfusion_matrix(y_test, y_pred)\n",
      "File \u001b[1;32mg:\\My Drive\\UdS\\Classes\\SNLP\\working\\solution.py:203\u001b[0m, in \u001b[0;36mtrain_and_fit_model\u001b[1;34m(corpus, classification_model)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_and_fit_model\u001b[39m(corpus: \u001b[39m\"\u001b[39m\u001b[39mtuple[list[str|int], list[str]]\u001b[39m\u001b[39m\"\u001b[39m, classification_model) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtuple[list[int], list[int]]\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    193\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m    Trains and fits a classification model using a given corpus.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39m        Tuple[List[int], List[int]]: A tuple containing the true labels and predicted labels for the test dataset.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m     corpus_features \u001b[39m=\u001b[39m vectorize_set(corpus[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m    204\u001b[0m     corpus_labels \u001b[39m=\u001b[39m corpus[\u001b[39m0\u001b[39m]\n\u001b[0;32m    205\u001b[0m     X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(corpus_features, corpus_labels, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "File \u001b[1;32mg:\\My Drive\\UdS\\Classes\\SNLP\\working\\solution.py:221\u001b[0m, in \u001b[0;36mvectorize_set\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39m# encode sentences as one-hot vectors using scikit-learn's one_hot function OHE\u001b[39;00m\n\u001b[0;32m    220\u001b[0m vectorizer \u001b[39m=\u001b[39m sklfe\u001b[39m.\u001b[39mCountVectorizer()\n\u001b[1;32m--> 221\u001b[0m \u001b[39mreturn\u001b[39;00m vectorizer\u001b[39m.\u001b[39;49mfit_transform(corpus)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1274\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1275\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1276\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 69\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# 7. Bigram Naive Bayes, polarized, include stop words\n",
    "solution = reload(solution)\n",
    "classification_model = MultinomialNB()\n",
    "corpus = solution.load_and_preprocess_data(polarize=True, ngramize=2)\n",
    "\n",
    "y_test, y_pred = solution.train_and_fit_model(corpus, classification_model)\n",
    "\n",
    "# Get the confusion matrix from solution.py file\n",
    "confusion_matrix = solution.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "solution.test(confusion_matrix, classifier=\"Naive Bayes\", preprocessing=\"Polairzed bigram with stop words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m classification_model \u001b[39m=\u001b[39m MultinomialNB()\n\u001b[0;32m      4\u001b[0m corpus \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39mload_and_preprocess_data(remove_stops\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, polarize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, ngramize\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m y_test, y_pred \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39;49mtrain_and_fit_model(corpus, classification_model)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Get the confusion matrix from solution.py file\u001b[39;00m\n\u001b[0;32m      9\u001b[0m confusion_matrix \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39mconfusion_matrix(y_test, y_pred)\n",
      "File \u001b[1;32mg:\\My Drive\\UdS\\Classes\\SNLP\\working\\solution.py:203\u001b[0m, in \u001b[0;36mtrain_and_fit_model\u001b[1;34m(corpus, classification_model)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_and_fit_model\u001b[39m(corpus: \u001b[39m\"\u001b[39m\u001b[39mtuple[list[str|int], list[str]]\u001b[39m\u001b[39m\"\u001b[39m, classification_model) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtuple[list[int], list[int]]\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    193\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m    Trains and fits a classification model using a given corpus.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39m        Tuple[List[int], List[int]]: A tuple containing the true labels and predicted labels for the test dataset.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m     corpus_features \u001b[39m=\u001b[39m vectorize_set(corpus[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m    204\u001b[0m     corpus_labels \u001b[39m=\u001b[39m corpus[\u001b[39m0\u001b[39m]\n\u001b[0;32m    205\u001b[0m     X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(corpus_features, corpus_labels, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "File \u001b[1;32mg:\\My Drive\\UdS\\Classes\\SNLP\\working\\solution.py:221\u001b[0m, in \u001b[0;36mvectorize_set\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39m# encode sentences as one-hot vectors using scikit-learn's one_hot function OHE\u001b[39;00m\n\u001b[0;32m    220\u001b[0m vectorizer \u001b[39m=\u001b[39m sklfe\u001b[39m.\u001b[39mCountVectorizer()\n\u001b[1;32m--> 221\u001b[0m \u001b[39mreturn\u001b[39;00m vectorizer\u001b[39m.\u001b[39;49mfit_transform(corpus)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1274\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1275\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1276\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 69\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# 8. Bigram Naive Bayes, polarized, exclude stop words\n",
    "solution = reload(solution)\n",
    "classification_model = MultinomialNB()\n",
    "corpus = solution.load_and_preprocess_data(remove_stops=True, polarize=True, ngramize=2)\n",
    "\n",
    "y_test, y_pred = solution.train_and_fit_model(corpus, classification_model)\n",
    "\n",
    "# Get the confusion matrix from solution.py file\n",
    "confusion_matrix = solution.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "solution.test(confusion_matrix, classifier=\"Naive Bayes\", preprocessing=\"Polairzed bigram without stop words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Bigram XGBoost, polaraized, include stop words\n",
    "solution = reload(solution)\n",
    "classification_model = XGBClassifier()\n",
    "corpus = solution.load_and_preprocess_data(polarize = True, ngramize=2)\n",
    "\n",
    "y_test, y_pred = solution.train_and_fit_model(corpus, classification_model)\n",
    "\n",
    "# Get the confusion matrix from solution.py file\n",
    "confusion_matrix = solution.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "solution.test(confusion_matrix, classifier=\"XGBoost\", preprocessing=\"Polairzed bigram with stop words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Bigram XGBoost, polarized, exclude stop words\n",
    "solution = reload(solution)\n",
    "classification_model = XGBClassifier()\n",
    "corpus = solution.load_and_preprocess_data(remove_stops=True, polarize=True, ngramize=2)\n",
    "\n",
    "y_test, y_pred = solution.train_and_fit_model(corpus, classification_model)\n",
    "\n",
    "# Get the confusion matrix from solution.py file\n",
    "confusion_matrix = solution.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "solution.test(confusion_matrix, classifier=\"XGBoost\", preprocessing=\"Polairzed bigram without stop words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Trigram Naive Bayes, polarized, include stop words\n",
    "solution = reload(solution)\n",
    "classification_model = MultinomialNB()\n",
    "corpus = solution.load_and_preprocess_data(polarize = True, ngramize=3)\n",
    "\n",
    "y_test, y_pred = solution.train_and_fit_model(corpus, classification_model)\n",
    "\n",
    "# Get the confusion matrix from solution.py file\n",
    "confusion_matrix = solution.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "solution.test(confusion_matrix, classifier=\"Naive Bayes\", preprocessing=\"Polairzed trigram with stop words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Trigram Naive Bayes, polarized, exclude stop words\n",
    "solution = reload(solution)\n",
    "classification_model = MultinomialNB()\n",
    "corpus = solution.load_and_preprocess_data(remove_stops=True, polarize=True, ngramize=3)\n",
    "\n",
    "y_test, y_pred = solution.train_and_fit_model(corpus, classification_model)\n",
    "\n",
    "# Get the confusion matrix from solution.py file\n",
    "confusion_matrix = solution.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "solution.test(confusion_matrix, classifier=\"Naive Bayes\", preprocessing=\"Polairzed bigram without stop words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Trigram XGBoost, polarized, include stop words\n",
    "solution = reload(solution)\n",
    "classification_model = XGBClassifier()\n",
    "corpus = solution.load_and_preprocess_data(polarize = True, ngramize=3)\n",
    "\n",
    "y_test, y_pred = solution.train_and_fit_model(corpus, classification_model)\n",
    "\n",
    "# Get the confusion matrix from solution.py file\n",
    "confusion_matrix = solution.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "solution.test(confusion_matrix, classifier=\"XGBoost\", preprocessing=\"Polairzed trigram with stop words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Trigram XGBoost, polarized, exclude stop words\n",
    "solution = reload(solution)\n",
    "classification_model = XGBClassifier()\n",
    "corpus = solution.load_and_preprocess_data(remove_stops=True, polarize=True, ngramize=3)\n",
    "\n",
    "y_test, y_pred = solution.train_and_fit_model(corpus, classification_model)\n",
    "\n",
    "# Get the confusion matrix from solution.py file\n",
    "confusion_matrix = solution.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "solution.test(confusion_matrix, classifier=\"XGBoost\", preprocessing=\"Polairzed trigram without stop words\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1bf99630d34a7296688ee6c5e32d155aeaeae4bf556023602f84a41a67fb9130"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
