{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLP Assignment 6 - Text Classification\n",
    "\n",
    "Name 1: William LaCroix<br/>\n",
    "Student id 1: 7038732<br/>\n",
    "Email 1: williamplacroix@gmail.com<br/>\n",
    "\n",
    "\n",
    "Name 2: Nicholas Jennings<br/>\n",
    "Student id 2: 2573492<br/>\n",
    "Email 2: s8nijenn@stud.uni-saarland.de<br/>\n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the respective Python files for any additional exercises necessary. There is no need to submit the data files should they exist. <br/>\n",
    "Upload the zipped folder on CMS. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Sentiment Analysis</span>\n",
    "\n",
    "   - Use the [FinancialPhraseBank](https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news) corpus with a 80:20 train test split, with a) a naive bayes b) XGBoost classifier for sentiment analysis (**2 points**)\n",
    "\n",
    "      - Naive Bayes: https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "      - XGBoost: https://xgboost.readthedocs.io/en/stable/index.html\n",
    "\n",
    "      - <span style=\"color:red\">Note:</span> Only use all_data.csv file for your experiments\n",
    "\n",
    "   - Use [LoughranMcDonald master dictionary](https://drive.google.com/file/d/17CmUZM9hGUdGYjCXcjQLyybjTrcjrhik/view?usp=sharing) to get word level polarity, replace words with the their corresponding polarity and now repeat the same classification task as above (**4 points**)\n",
    "\n",
    "      - What happens if you remove the stop words? \n",
    "\n",
    "      - What's the ratio of stopwords (taken from NLTK) amongst the total word count? \n",
    "\n",
    "      - Is it a good choice to do stopword removal? Explain with 2-3 examples why or why not?\n",
    "   \n",
    "   - Create cascaded Bi-Grams and tri-grams and repeat both the exercises from above. Which representation do you think is a better value in terms of accuracy and computational power required? (**3 points**)\n",
    "\n",
    "      - Bi-grams example: \n",
    "\n",
    "         - Company A barely surpassed their profit expectations.\n",
    "         \n",
    "         - (Company, A), (A, barely), (barely, surpassed), (surpassed, their) ...\n",
    "   \n",
    "   - How would you represnt the polarity in uni/bi/tri-gram models with Huffman Encoding? (**1 point**)\n",
    "\n",
    "\n",
    "### Few points to remember\n",
    "   - While splitting your dataset, use seed=42\n",
    "   - Use type hint(s) in your code, and add a docstring to your functions or classes\n",
    "   - Focus on the readability of your code, that helps us to give you better feedback on where the code went wrong\n",
    "   - <span style=\"color:red\">Do not submit the data or dictionary file.</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import solution\n",
    "from importlib import reload\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "solution = reload(solution)\n",
    "\n",
    "#ToDo: Add code here to call the functions for each step as you did in assignment4,\n",
    "# i.e. loading the dataset, doing the splits, etc.\n",
    "corpus = solution.load_and_preprocess_data(\"./all-data.csv\", remove_punct=True, lowercase=True)\n",
    "\n",
    "corpus_features = solution.vectorize_set(corpus[1])\n",
    "\n",
    "corpus_labels = corpus[0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus_features, corpus_labels, test_size=0.2)#, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  negative  neutral  positive  All\n",
      "True                                       \n",
      "negative         56       30        25  111\n",
      "neutral          16      512        72  600\n",
      "positive         11      118       130  259\n",
      "All              83      660       227  970\n",
      "\n",
      "Naive Bayes accuracy: 0.5027027027027027\n",
      "Naive Bayes precision: 0.5019305019305019\n",
      "Naive Bayes recall: 0.7027027027027027\n",
      "Naive Bayes F1: 0.5855855855855856\n"
     ]
    }
   ],
   "source": [
    "solution = reload(solution)\n",
    "gnb = MultinomialNB()\n",
    "NB_y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Get the confusion matrix from solution.py file, since it's a multi-class classification\n",
    "# Confusion matrix should give you a better idea of how well your model is performing\n",
    "# Naive Bayes classification of 3 class labels\n",
    "# NB_confusion_matrix on 3x3 matrix\n",
    "NB_confusion_matrix = solution.confusion_matrix(y_test, NB_y_pred)\n",
    "print(NB_confusion_matrix)\n",
    "solution.test(NB_confusion_matrix, \"Naive Bayes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus features and labels mapped\n",
      "corpus split\n",
      "model created\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m xgb_model \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBClassifier(objective\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmulti:softprob\u001b[39m\u001b[39m\"\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmodel created\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m xgb_y_pred \u001b[39m=\u001b[39m tqdm(xgb_model\u001b[39m.\u001b[39;49mfit(xgb_X_train, xgb_y_train)\u001b[39m.\u001b[39mpredict(xgb_X_test))\n\u001b[0;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmodel fit\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[39m# Get the confusion matrix from solution.py file, since it's a multi-class classification\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m# Confu sion matrix should give you a better idea of how well your model is performing\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m# XGBoost classification of 3 class labels\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m# xgb_confusion_matrix on 3x3 matrix\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1462\u001b[0m (\n\u001b[0;32m   1463\u001b[0m     model,\n\u001b[0;32m   1464\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1470\u001b[0m )\n\u001b[0;32m   1471\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1472\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[0;32m   1473\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[0;32m   1488\u001b[0m )\n\u001b[1;32m-> 1490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1491\u001b[0m     params,\n\u001b[0;32m   1492\u001b[0m     train_dmatrix,\n\u001b[0;32m   1493\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1494\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1495\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1496\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1497\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1498\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1499\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1500\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1501\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1502\u001b[0m )\n\u001b[0;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[0;32m   1505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[0;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[0;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "import tqdm\n",
    "\n",
    "solution = reload(solution)\n",
    "\n",
    "xgb_corpus = corpus.copy()\n",
    "xgb_corpus[0] = xgb_corpus[0].replace(\"positive\", 2)\n",
    "xgb_corpus[0] = xgb_corpus[0].replace(\"neutral\", 1)\n",
    "xgb_corpus[0] = xgb_corpus[0].replace(\"negative\", 0)\n",
    "\n",
    "xgb_corpus_features = solution.vectorize_set(xgb_corpus[1])\n",
    "xgb_corpus_labels = xgb_corpus[0]\n",
    "xgb_X_train, xgb_X_test, xgb_y_train, xgb_y_test = train_test_split(xgb_corpus_features, xgb_corpus_labels, test_size=0.2)#, random_state=42)\n",
    "xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42)\n",
    "xgb_y_pred = xgb_model.fit(xgb_X_train, xgb_y_train).predict(xgb_X_test)\n",
    "\n",
    "# Get the confusion matrix from solution.py file, since it's a multi-class classification\n",
    "# Confu sion matrix should give you a better idea of how well your model is performing\n",
    "# XGBoost classification of 3 class labels\n",
    "# xgb_confusion_matrix on 3x3 matrix\n",
    "xgb_confusion_matrix = solution.confusion_matrix(xgb_y_test, xgb_y_pred)\n",
    "print(xgb_confusion_matrix)\n",
    "solution.test(xgb_confusion_matrix, \"XGBoost\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoughranMcDonald\n",
    "solution = reload(solution)\n",
    "lm = #TODO: Add code here to call the LoughranMcDonald classifier\n",
    "lm_y_pred = lm.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Get the confusion matrix from solution.py file, since it's a multi-class classification\n",
    "# Confusion matrix should give you a better idea of how well your model is performing\n",
    "# XGBoost classification of 3 class labels\n",
    "# xgb_confusion_matrix on 3x3 matrix\n",
    "lm_confusion_matrix = solution.confusion_matrix(y_test, lm_y_pred)\n",
    "print(lm_confusion_matrix)\n",
    "solution.test(lm_confusion_matrix, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Naive Bayes\n",
    "\n",
    "# Bigram XGBoost\n",
    "\n",
    "# Bigram LoughranMcDonald\n",
    "\n",
    "# Trigram Naive Bayes\n",
    "\n",
    "# Trigram XGBoost\n",
    "\n",
    "# Trigram LoughranMcDonald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train), type(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1bf99630d34a7296688ee6c5e32d155aeaeae4bf556023602f84a41a67fb9130"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
