{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLP Assignment 7 - Word Sense Disambiguation\n",
    "\n",
    "Name 1: William LaCroix<br/>\n",
    "Student id 1: 7038732<br/>\n",
    "Email 1: williamplacroix@gmail.com<br/>\n",
    "\n",
    "\n",
    "Name 2: Nicholas Jennings<br/>\n",
    "Student id 2: 2573492<br/>\n",
    "Email 2: s8nijenn@stud.uni-saarland.de<br/>\n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the respective Python files for any additional exercises necessary. There is no need to submit the data files should they exist. <br/>\n",
    "Upload the zipped folder on CMS. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">EM for Word Sense Disambiguation</span>\n",
    "\n",
    "In this exercise, you will implement [expectation-maximization](https://machinelearningmastery.com/expectation-maximization-em-algorithm/) for word sense disambiguation.\n",
    "\n",
    "\n",
    "There is no starter code provided for this exercise, and you have to write your code from scratch. We have however provided some pseudo-code in this notebook.\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "NLTK has a corpus reader for the Senseval 2 dataset. This data set contains data for four ambiguous words: hard, line, serve and interest. You can read more here: http://www.nltk.org/howto/corpus.html (search for ”senseval”). The provided code loads the dataset for you and converts each sample into a sample object. The sample class has two attributes: context and label. Label is the ground truth sense of the ambiguous word. As EM is an unsupervised method, we will only use the labels for final evaluation. Context is the left and right context of the ambiguous word as word IDs: it is a list of integers, which you can use later to index matrices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo Code\n",
    "\n",
    "```\n",
    "instances = senseval.instances(hard_f)\n",
    "\n",
    "# All training samples as a list.\n",
    "samples = [sample(inst) for inst in instances]\n",
    "\n",
    "# Convert contexts to indices so they can be used for indexing.\n",
    "for sample in samples:\n",
    "    sample.context_to_index(word_to_id)\n",
    "```\n",
    "\n",
    "Now, ”samples” contains all the sample objects for the ambiguous word\n",
    "”hard”."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "There are three matrices provided for you, which you will have to use for the implementation: $priors$, $probs$ and $C$.\n",
    "\n",
    " - $priors$: It is a vector of length K, where K is the number of clusters. Each value corresponds to a cluster prior $p(s_k)$. It’s initialized randomly.\n",
    "\n",
    " - $probs$: It is a V x K sized matrix, where V is the size of the vocabulary. Each column is a conditional probability distribution over the words. $probs[i, k]$ is $p(v_i |s_k )$.\n",
    "\n",
    " - $C$ : contains the counts of the words in a given context. The size of the matrix is number of samples x vocabulary size. $C[i, j]$ is $C(v_j~in~c_i )$, the count of word j in context i.\n",
    "\n",
    "\n",
    "Get the IDs of the words. We will use this to index the rows of the probs matrix:\n",
    "\n",
    "```\n",
    "context_index = sample.context\n",
    "words_given_sense = probs[context.index, :]\n",
    "```\n",
    "\n",
    "This is a matrix: the number of lines is the number of words in the context, the number of columns is number of senses. We multiply this column wise to get the probability of a context given a sense $p(c_i |s_k )$:\n",
    "\n",
    "```\n",
    "context_given_sense = np.prod(words_given_sense, axis=0)\n",
    "```\n",
    "\n",
    "This is a vector where each value is a class conditional probability (size is K)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    " - Briefly describe what EM is and when using it is appropriate? Give an example use case (1 point).\n",
    "\n",
    " - Write the code for the expectation step (3 points).\n",
    "\n",
    " - Write the code for the maximization step. In order to check your implementation, make sure that the log likelihood increases over the iterations (4 points).\n",
    "\n",
    " - The code will print the ordered frequency of the labels within each cluster. Each cluster corresponds to one of the senses of \"hard\". How does the algorithm perform? Briefly explain. (1 point)\n",
    "\n",
    " - Does EM find the global optimum? Explain why/why not? (1 point)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1bf99630d34a7296688ee6c5e32d155aeaeae4bf556023602f84a41a67fb9130"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
